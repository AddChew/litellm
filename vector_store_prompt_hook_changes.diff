diff --git a/litellm/integrations/vector_store_prompt_hook.py b/litellm/integrations/vector_store_prompt_hook.py
new file mode 100644
index 0000000..1234567
--- /dev/null
+++ b/litellm/integrations/vector_store_prompt_hook.py
@@ -0,0 +1,271 @@
+"""
+Vector Store Prompt Hook Integration
+
+This module provides a direct integration for vector store search within chat completion prompt hooks.
+It uses the functions from litellm.vector_stores.main directly rather than going through custom loggers.
+"""
+
+import time
+from typing import Dict, List, Optional, Tuple
+
+from litellm._logging import verbose_logger
+from litellm.integrations.custom_logger import CustomLogger
+from litellm.types.llms.openai import AllMessageValues
+from litellm.types.utils import (
+    LiteLLMLoggingObj,
+    StandardCallbackDynamicParams,
+    StandardLoggingVectorStoreRequest,
+)
+
+
+class VectorStorePromptHook(CustomLogger):
+    """
+    A prompt hook that performs vector store search using main.py functions directly.
+    
+    This integration:
+    - Extracts user queries from chat messages
+    - Searches specified vector stores using litellm.vector_stores.main functions
+    - Injects search results as context into user messages
+    - Logs vector store requests in standard logging format
+    """
+
+    def __init__(self, **kwargs):
+        super().__init__(**kwargs)
+
+    async def async_get_chat_completion_prompt(
+        self,
+        model: str,
+        messages: List[AllMessageValues],
+        non_default_params: dict,
+        prompt_id: Optional[str],
+        prompt_variables: Optional[dict],
+        dynamic_callback_params: StandardCallbackDynamicParams,
+        litellm_logging_obj: LiteLLMLoggingObj,
+        tools: Optional[List[Dict]] = None,
+        prompt_label: Optional[str] = None,
+        prompt_version: Optional[int] = None,
+    ) -> Tuple[str, List[AllMessageValues], dict]:
+        """
+        Async version of the vector store search prompt hook.
+        
+        Returns:
+            Tuple of (model, modified_messages, modified_non_default_params)
+        """
+        # Check if vector store search is enabled
+        vector_store_search_enabled = non_default_params.get("vector_store_search_enabled", False)
+        vector_store_ids = non_default_params.get("vector_store_ids", [])
+        
+        if not vector_store_search_enabled or not vector_store_ids:
+            return model, messages, non_default_params
+        
+        try:
+            # Import the vector store search function from main.py
+            from litellm.vector_stores.main import asearch
+            from litellm.litellm_core_utils.prompt_templates.common_utils import get_last_user_message
+            
+            # Extract the user's query from the last message
+            user_query = get_last_user_message(messages)
+            
+            if not user_query:
+                return model, messages, non_default_params
+            
+            # Initialize metadata for logging vector store requests
+            vector_store_request_metadata: List[StandardLoggingVectorStoreRequest] = []
+            search_results_content = []
+            
+            # Search each vector store
+            for vector_store_id in vector_store_ids:
+                try:
+                    start_time = time.time()
+                    
+                    # Perform the vector store search
+                    search_response = await asearch(
+                        vector_store_id=vector_store_id,
+                        query=user_query,
+                        custom_llm_provider=non_default_params.get("custom_llm_provider", "openai"),
+                        litellm_logging_obj=litellm_logging_obj,
+                        max_num_results=non_default_params.get("max_num_results", 10),
+                        filters=non_default_params.get("filters"),
+                        ranking_options=non_default_params.get("ranking_options"),
+                        rewrite_query=non_default_params.get("rewrite_query"),
+                    )
+                    
+                    end_time = time.time()
+                    
+                    # Extract content from search results
+                    if search_response and hasattr(search_response, 'data'):
+                        for result in search_response.data:
+                            if hasattr(result, 'content'):
+                                for content_item in result.content:
+                                    if hasattr(content_item, 'text'):
+                                        search_results_content.append(content_item.text)
+                    
+                    # Create metadata for logging
+                    vector_store_metadata = StandardLoggingVectorStoreRequest(
+                        vector_store_id=vector_store_id,
+                        custom_llm_provider=non_default_params.get("custom_llm_provider", "openai"),
+                        query=user_query,
+                        vector_store_search_response=search_response,
+                        start_time=start_time,
+                        end_time=end_time,
+                    )
+                    vector_store_request_metadata.append(vector_store_metadata)
+                    
+                except Exception as e:
+                    # Log the error but continue with other vector stores
+                    verbose_logger.warning(f"Error searching vector store {vector_store_id}: {str(e)}")
+                    continue
+            
+            # If we found search results, append them to the messages
+            if search_results_content:
+                context_content = "\n\nContext from vector store search:\n" + "\n".join(search_results_content)
+                
+                # Find the last user message and append context
+                for i in range(len(messages) - 1, -1, -1):
+                    if messages[i].get("role") == "user":
+                        if isinstance(messages[i]["content"], str):
+                            messages[i]["content"] += context_content
+                        elif isinstance(messages[i]["content"], list):
+                            # Handle list content (multimodal)
+                            messages[i]["content"].append({
+                                "type": "text",
+                                "text": context_content
+                            })
+                        break
+            
+            # Store vector store metadata for logging
+            if vector_store_request_metadata:
+                metadata = non_default_params.get("metadata", {})
+                metadata["vector_store_request_metadata"] = vector_store_request_metadata
+                non_default_params["metadata"] = metadata
+                
+        except Exception as e:
+            # Log the error but don't fail the request
+            verbose_logger.warning(f"Error in vector store search prompt hook: {str(e)}")
+        
+        return model, messages, non_default_params
+
+    def get_chat_completion_prompt(
+        self,
+        model: str,
+        messages: List[AllMessageValues],
+        non_default_params: dict,
+        prompt_id: Optional[str],
+        prompt_variables: Optional[dict],
+        dynamic_callback_params: StandardCallbackDynamicParams,
+        prompt_label: Optional[str] = None,
+        prompt_version: Optional[int] = None,
+    ) -> Tuple[str, List[AllMessageValues], dict]:
+        """
+        Sync version of the vector store search prompt hook.
+        
+        Returns:
+            Tuple of (model, modified_messages, modified_non_default_params)
+        """
+        # Check if vector store search is enabled
+        vector_store_search_enabled = non_default_params.get("vector_store_search_enabled", False)
+        vector_store_ids = non_default_params.get("vector_store_ids", [])
+        
+        if not vector_store_search_enabled or not vector_store_ids:
+            return model, messages, non_default_params
+        
+        try:
+            # Import the vector store search function from main.py
+            from litellm.vector_stores.main import search
+            from litellm.litellm_core_utils.prompt_templates.common_utils import get_last_user_message
+            
+            # Extract the user's query from the last message
+            user_query = get_last_user_message(messages)
+            
+            if not user_query:
+                return model, messages, non_default_params
+            
+            # Initialize metadata for logging vector store requests
+            vector_store_request_metadata: List[StandardLoggingVectorStoreRequest] = []
+            search_results_content = []
+            
+            # Search each vector store
+            for vector_store_id in vector_store_ids:
+                try:
+                    start_time = time.time()
+                    
+                    # Perform the vector store search (sync version)
+                    search_response = search(
+                        vector_store_id=vector_store_id,
+                        query=user_query,
+                        custom_llm_provider=non_default_params.get("custom_llm_provider", "openai"),
+                        litellm_logging_obj=None,  # Not available in sync version
+                        max_num_results=non_default_params.get("max_num_results", 10),
+                        filters=non_default_params.get("filters"),
+                        ranking_options=non_default_params.get("ranking_options"),
+                        rewrite_query=non_default_params.get("rewrite_query"),
+                    )
+                    
+                    end_time = time.time()
+                    
+                    # Extract content from search results
+                    if search_response and hasattr(search_response, 'data'):
+                        for result in search_response.data:
+                            if hasattr(result, 'content'):
+                                for content_item in result.content:
+                                    if hasattr(content_item, 'text'):
+                                        search_results_content.append(content_item.text)
+                    
+                    # Create metadata for logging
+                    vector_store_metadata = StandardLoggingVectorStoreRequest(
+                        vector_store_id=vector_store_id,
+                        custom_llm_provider=non_default_params.get("custom_llm_provider", "openai"),
+                        query=user_query,
+                        vector_store_search_response=search_response,
+                        start_time=start_time,
+                        end_time=end_time,
+                    )
+                    vector_store_request_metadata.append(vector_store_metadata)
+                    
+                except Exception as e:
+                    # Log the error but continue with other vector stores
+                    verbose_logger.warning(f"Error searching vector store {vector_store_id}: {str(e)}")
+                    continue
+            
+            # If we found search results, append them to the messages
+            if search_results_content:
+                context_content = "\n\nContext from vector store search:\n" + "\n".join(search_results_content)
+                
+                # Find the last user message and append context
+                for i in range(len(messages) - 1, -1, -1):
+                    if messages[i].get("role") == "user":
+                        if isinstance(messages[i]["content"], str):
+                            messages[i]["content"] += context_content
+                        elif isinstance(messages[i]["content"], list):
+                            # Handle list content (multimodal)
+                            messages[i]["content"].append({
+                                "type": "text",
+                                "text": context_content
+                            })
+                        break
+            
+            # Store vector store metadata for logging
+            if vector_store_request_metadata:
+                metadata = non_default_params.get("metadata", {})
+                metadata["vector_store_request_metadata"] = vector_store_request_metadata
+                non_default_params["metadata"] = metadata
+                
+        except Exception as e:
+            # Log the error but don't fail the request
+            verbose_logger.warning(f"Error in vector store search prompt hook: {str(e)}")
+        
+        return model, messages, non_default_params
+
+    @property
+    def integration_name(self) -> str:
+        """Return the integration name for identification."""
+        return "vector_store_prompt_hook"
+
+
+# Helper function to check if vector store prompt hook should be enabled
+def should_enable_vector_store_prompt_hook(non_default_params: Dict) -> bool:
+    """
+    Check if the vector store prompt hook should be enabled based on the parameters.
+    
+    Args:
+        non_default_params: The non-default parameters from the completion call
+        
+    Returns:
+        True if the hook should be enabled, False otherwise
+    """
+    return (
+        non_default_params.get("vector_store_search_enabled", False) and
+        bool(non_default_params.get("vector_store_ids", []))
+    )

diff --git a/litellm/litellm_core_utils/litellm_logging.py b/litellm/litellm_core_utils/litellm_logging.py
index 1234567..8901234 100644
--- a/litellm/litellm_core_utils/litellm_logging.py
+++ b/litellm/litellm_core_utils/litellm_logging.py
@@ -59,6 +59,10 @@ from litellm.integrations.vector_store_integrations.bedrock_vector_store import
     BedrockVectorStore,
 )
+from litellm.integrations.vector_store_prompt_hook import (
+    VectorStorePromptHook,
+    should_enable_vector_store_prompt_hook,
+)
 from litellm.litellm_core_utils.get_litellm_params import get_litellm_params
 from litellm.litellm_core_utils.llm_cost_calc.tool_call_cost_tracking import (
     StandardBuiltInToolCostTracking,
@@ -538,6 +542,12 @@ class Logging(LiteLLMLoggingBaseClass):
 
         eg. AnthropicCacheControlHook and BedrockKnowledgeBaseHook both don't require a `prompt_id` to be passed in, they are triggered by dynamic params
         """
+        # Check for vector store prompt hook (new main.py approach)
+        if should_enable_vector_store_prompt_hook(non_default_params):
+            return True
+
         for param in non_default_params:
             if param in DynamicPromptManagementParamLiteral.list_all_params():
                 return True
@@ -656,6 +666,12 @@ class Logging(LiteLLMLoggingBaseClass):
         Returns:
             A CustomLogger instance if one is found, None otherwise
         """
+        #########################################################
+        # NEW: Vector Store Prompt Hook (using main.py functions)
+        #########################################################
+        if should_enable_vector_store_prompt_hook(non_default_params):
+            vector_store_prompt_hook = VectorStorePromptHook()
+            self.model_call_details["prompt_integration"] = "vector_store_prompt_hook"
+            return vector_store_prompt_hook
+
         # First check if model starts with a known custom logger compatible callback
         for callback_name in litellm._known_custom_logger_compatible_callbacks:
             if model.startswith(callback_name):
@@ -689,7 +705,7 @@ class Logging(LiteLLMLoggingBaseClass):
             return anthropic_cache_control_logger
 
         #########################################################
-        # Vector Store / Knowledge Base hooks
+        # Vector Store / Knowledge Base hooks (existing registry approach)
         #########################################################
         if litellm.vector_store_registry is not None:
             if vector_store_to_run := litellm.vector_store_registry.get_vector_store_to_run(

diff --git a/litellm/litellm_core_utils/custom_logger_registry.py b/litellm/litellm_core_utils/custom_logger_registry.py
index 1234567..8901234 100644
--- a/litellm/litellm_core_utils/custom_logger_registry.py
+++ b/litellm/litellm_core_utils/custom_logger_registry.py
@@ -37,6 +37,7 @@ from litellm.integrations.sqs import SQSLogger
 from litellm.integrations.vector_store_integrations.bedrock_vector_store import (
     BedrockVectorStore,
 )
+from litellm.integrations.vector_store_prompt_hook import VectorStorePromptHook
 from litellm.proxy.hooks.dynamic_rate_limiter import _PROXY_DynamicRateLimitHandler


@@ -77,6 +78,7 @@ class CustomLoggerRegistry:
         "anthropic_cache_control_hook": AnthropicCacheControlHook,
         "agentops": AgentOps,
         "bedrock_vector_store": BedrockVectorStore,
+        "vector_store_prompt_hook": VectorStorePromptHook,
         "deepeval": DeepEvalLogger,
         "s3_v2": S3Logger,
         "aws_sqs": SQSLogger,